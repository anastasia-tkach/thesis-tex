% !TEX root = ../htrack.tex
\section{Overview}
\label{sec:htrack-overview}

\input{htrack/fig/wristband.tex}

% - input data from sensors, illustrate how the data sucks
% - our hand model, motivation of why we use the cylinders
% - output, i.e. pose parameters
% - short paragraphs on the registration formulation and the optimization
% - highlight contributions again
Robust hand tracking with a commodity depth sensor is highly challenging due to self-occlusion, low quality/density of sensor data and the high degree of articulation of the human hand.
We address these issues by proposing a regularized articulated ICP-like optimization that carefully balances data fitting with suitable priors (\Figure{overview}). 
%
Our data fitting performs a joint \emph{2D-3D} optimization. 
% \MP{Would be good to add an overview figure that shows the different components and notation. Input data, preprocessing, registration optimization with different terms, data prior, output data.}
The 3D alignment ensures that every point measured by the sensor is sufficiently close to the tracked model $\handmodel$.
% \MP{We should be consistent about notation. Later, $\PointsSensor$ is a point cloud, here it looks like it's the sensor or a point from the sensor. It's probably easiest not to introduce the notation in this paragraph.}
Simultaneously, as we cannot create such constraints for occluded parts of the hand, we integrate a 2D registration that pushes the tracked model to lie within the sensor visual hull. A carefully chosen set of priors regularizes the solution to ensure the recovered pose is plausible. 

% We determined that retaining realistic hand postures is critical, as erroneous postures can result in establishing erroneous closest-point correspondences and cause catastrophic loss of tracking. \MP{not clear to me. a realistic pose can just as well lead to wrong correspondences. I would probably skip this sentence.}


\subsection*{Acquisition device}
Our system processes raw data acquired at 60 fps from a single RGBD sensor. \Figure{data} illustrates this data for the \emph{PrimeSense Carmine 1.09} structured light sensor as well as the \emph{Creative Gesture Camera} time-of-flight sensor. From the raw data our algorithm extracts a 2D \emph{silhouette image} $\SilhoSensor$ and a 3D \emph{point cloud} $\PointsSensorHtrack$. The two sensors exhibit different types of imperfections. The precision of depth measurements in the PrimeSense camera is significantly higher. However, substantial holes often occur at grazing angles, e.g.\ note the gap in the data where we would expect to see the index finger. Conversely, the Creative Gesture Camera provides an accurate and gap-free silhouette image, but suffers from high noise in the depth measurements, therefore resulting in very noisy point clouds. Our algorithm is designed to handle both types of imperfections. This is achieved by formulating an optimization that \emph{jointly} considers silhouette and point cloud, balancing their contribution in a way that conforms to the quality of sensor data.

\subsection*{Tracking model}
Our algorithm registers a template hand model to the sensor data. Similar to other techniques~\cite{oiko2011hand,schroder2014real}, we employ a simple (sphere capped) cylinder model as a geometric template; see \Figure{handmodel}.  We optimize for $26$ degrees of freedom, $6$ for global rotation and translation and $20$ for articulation.
% 
% \MP{We should say something about how the hand is customized to the user. Right now, it appears as if one generic model would work for all people.}
Like in \cite{melax2013dynamics}, the model can be quickly adjusted to the user by specifying global scale, palm size and finger lengths. In most scenarios, it is sufficient to perform a simple uniform scaling of the model.
% 
Such a coarse geometry is sufficient for hand tracking, as the signal-to-noise ratio for commercially available RGBD sensors is low for samples on the fingers when compared to the size of a finger. Furthermore, the computation of closest-point correspondences can be performed in closed form and in parallel, which is essential for real-time performance.
% 
\revision{The hand's palm region may be better approximated by geometries other than a cylinder, but we found using only cylinder primitives to work well for tracking in terms of accuracy and efficiency. Furthermore, it simplified the implementation as the same correspondence computation routine can be used for all primitives in the model.}
% 
While the geometry of the model used for tracking remains coarse, our algorithm computes joint angles (including rigid transformation) in the widespread \emph{BVH} motion sequence format; these can be used to drive a high-resolution skinned hand rig as illustrated in \Figure{handmodel}-d.
%

%\section{Technical details}
\label{sec:tech}
\subsection*{Preprocessing}
The  silhouette image $\SilhoSensor$ is not directly available from the sensor and needs to be computed. This labeling can be obtained by extracting  the sensor color image and performing a skin color segmentation~\cite{oiko_cvpr12,schroder2014real}, or can be obtained directly from depth images by performing a classification with randomized forests~\cite{tompson2014real}. Another possibility is to exploit a full-body tracking algorithm~\cite{shotton_cvpr11} and segment the hand according to the wrist position. For gestural tracking, where the hand is typically the closest object to the sensor~\cite{qian2014realtime}, a black wristband can be used to simplify segmentation by creating a gap in the depth image. Similarly to this method, in our system the user wears a \emph{colored wristband}. We first identify the position of the wristband in the scene by color segmentation,
then retrieve the 3D points in the proximity of the wristband and compute the principal axis. This axis, in conjunction with the wristband centroid, is then used to segment the hand point cloud. Any depth pixel within the hand point cloud is labelled as belonging to the silhouette image $\SilhoSensor$ as shown in \Figure{wristband}.
% \MP{We should add an image here.} DONE



%%% Local Variables:  
%%% mode: latex 
%%% TeX-master: "../htrack" 
%%% End: 
