\chapter{Conclusion}

\section{Summary}

In Chapter \ref{ch:tracking} we have presented a new model-based approach to realtime hand tracking using a single low-cost depth camera. This simple acquisition setup maximizes ease of deployment, but poses significant challenges for robust tracking. 
Our analysis revealed that a major source of error when tracking articulated hands are erroneous correspondences between the hand model and the acquired data, mainly caused by outliers, holes, or data popping in and out during acquisition. 
We demonstrate that these problems can be resolved by our new formulation of correspondence search. In combination with suitable 2D/3D registration energies and data-driven priors, this leads to a robust and efficient hand tracking algorithm that outperforms existing model- and appearance-based solutions. In our experiments we show that our system runs seamlessly for sensors capturing data at 60 Hz. 

In Chapter \ref{ch:sphere-meshes} we have introduced the use of sphere-meshes as a novel geometric representation for articulated tracking. We have demonstrated how this representation yields excellent results for real-time registration of articulated geometry, and presented a calibration algorithm to estimate a per-user tracking template. We have validated our results by demonstrating qualitative as well as quantitative improvements over the state-of-the-art. Our calibration optimization is related to the works of \cite{taylor2014user},  \cite{khamis15learning} and  \cite{joseph2016fits}, with a fundamental difference: the innate simplicity of sphere-meshes substantially simplifies the algorithmic complexity of calibration and tracking algorithms. This considered, we believe that with the use of compute shaders, articulated tracking on the GPU can become as effortless and efficient as simple mesh rasterization. 
Our sphere-mesh models are a first approximations to the implicit functions lying at the core of the recently proposed geometric skinning techniques \cite{vaillant2013implicit},  \cite{vaillant2014robust}. Therefore, we believe the calibration of sphere-meshes to be the first step towards photorealistic real-time hand modeling and tracking.

In Chapter \ref{ch:online} we have presented the first accurate online hand calibration system. 
From an application point of view, our approach significantly improves on the usability of real-time hand tracking, as it requires neither controlled calibration scans nor offline processing prior to tracking. This allows easy deployment in consumer-level applications. From a technical point of view, we introduce a principled approach to online integration of shape information of user-specific hand geometry. By leveraging uncertainty estimates derived from the optimization objective function, we automatically determine how informative each input frame is for improving the estimates of the different unknown model parameters. Our approach is general and can be applied to different types of calibration, e.g., for full body tracking. More broadly, we envisage applications to other difficult types of model estimation problems, where unreliable data needs to be accumulated and integrated into a consistent representation.

By fully disclosing our source code and data we ensure that our method and results are reproducible, as well as facilitating future research and product development. 

\section {Future Work}

\subsection*{Discriminative Tracking}
Our previous work was focused on generative component of a hand tracking system. For re-initialization we used a simple algorithm, similar to the one proposed by \cite{qian2014realtime}. Meanwhile, discriminative tracking quality and efficiency was greatly improved in recent works. 
The use of more advanced re-initialization techniques like \cite{oberweger2017deepprior++} would benefit our system. Further, we believe an interesting venue for future work is how to elegantly integrate per-frame estimates into generative trackers. State-of-the-art discriminative algorithms regress a full hand pose; however, a generative component of a system might only require predicting the palm transformation to re-initialize, similar to \cite{taylor2017articulated}. It is instructive to research what would be an optimal discriminative input for a model-based tracker.

\subsection*{Hand Calibration for CNN}
In discriminative tracking a costly and time-consuming part is annotating the training data. Recently \cite{ dibra2017refine} presented a system that eliminates the need in annotation. They use a differentiable rendering algorithm and predict hand pose by training a CNN to match a synthesized depth and an input depth. The accuracy of this system could be enhanced by regressing hand \textit{shape} in addition to hand \textit{pose}. In previous work we developed a shape-parametrized sphere-meshes hand template which could be used for this purpose.

\subsection*{Hand Tracking from RGB}
Currently our system is only using depth images as an input. In the future we could incorporate an RGB-based term in our optimization. An RGB input is complementary to depth data, since it does not contain holes and sensor noise. Moreover, it provides higher quality edges and finer texture details. For example, \cite{weise2011realtime} have proposed an optical flow constraints energy term. We could use a similar energy as a starting point of our experiments. The discriminative component of hand tracking system can also benefit from being augmented with RGB input. Recently the first hand-tracking approaches that require RGB-only input were presented \cite{simon2017hand}, \cite{zimmermann2017learning}. These techniques could be further explored and combined with standard depth input.

\subsection*{Improving Efficiency}
In future work we plan to reduce computational overhead to facilitate deployment on mobile devices. In our system the closest point correspondences are computed independently for each point, thus the algorithm has a parallelizable structure. The run time can be decreased in several ways: through the use of compute shaders that eliminate context switch time between CUDA and OpenGL; via on-chip implementation of the algorithm; or through higher downsampling factors of the input data (possibly with adaptive rates depending on the hand part).

\subsection*{Feedback for Hand Calibration}
To obtain a complete personalized tracking model, the user needs to perform a suitable series of hand poses. As discussed above, if a finger is never bent, the estimate of phalanx lengths will be unreliable. Currently, the system provides limited visual feedback to the user to guide the calibration. In the future, we aim to design a feedback system that provides visual indication of the most informative hand poses given the current model estimate. For example, one could create a dictionary containing a suitable pose for estimating each parameter with high certainty. During calibration the user is prompted to show hand pose corresponding to the lowest certainty parameter. 

\subsection*{Tracking Hands and Body}
Other interesting avenues for future work include adapting the method to other tracking scenarios, such as full body tracking. The first \textit{offline} hands and body tracking system was recently introduced by \cite{romero2017embodied}. We envision full-body avatars used for communication as a primary application of such methods, which necessitates interactive frame rates. Approaches like \cite{ dou2017motion2fusion} are developed for high frame rates, however they struggle handling high deformation exhibited during hand motion.

\subsection*{Dynamic Fusion with Sphere Meshes}
The topology of our sphere-meshes template has been defined in a manual trial-and-error process. A more suitable topology could be estimated by optimization, possibly even adapting the topology for specific users; For example, the work in \cite{thiery2016spheremesh} could be extended to space-time point clouds. Similarly, one could think of a variant of \cite{newcombe2015dynfusion} where sphere-meshes are instantiated on-the-fly. 

\subsection*{Two hands and Hand-object Interactions}
Building upon the basis of high-precision generative hand tracking developed in our previous works, one could extend the system to enable two hands or hand-object interaction. The main component to be added is a discriminative system that labels the pixels of the input image as right hand, left hand, object or background. Another crucial modification would be adjusting the optimized energy to work robustly under heavy occlusion. Excellent works in two-hand and hand-object interaction were recently presented -  \cite{taylor2017articulated}, \cite{mueller2017real}. However, this is still a very new area with room for improvement in accuracy.

\subsection*{Photorealistic Hands Avatars}
Photorealistic hands could be used as a part of body avatar for communication or for increasing immersion in virtual reality applications. The systems for creating photorealistic face avatars were already proposed, for example \cite{thies2016face2face} and \cite{tewari2017mofa}. In our work we have already developed a parametric model of hand pose and shape. For creating hand avatars similarly to\cite{thies2016face2face} and \cite{tewari2017mofa}, we need to also create a parametric model of hand texture. Alternatively, the texture along with fine geometric details could accumulated from RGD input during the calibration sequence.

\subsection*{Natural User Interfaces}
Integrating our hand tracking solution to natural user interfaces and testing its capabilities for interaction with virtual objects could drive further research directions. Through this kind of testing we could learn which components require improvement and which are not so crucial for the control applications.
