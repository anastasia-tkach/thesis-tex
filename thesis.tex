\input{head/settings_epfl_template.tex}

%\documentclass{egpubl}
%\usepackage{egsgp15}
%\usepackage{t1enc,dfadobe}
%\usepackage{egweblnk}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{microtype}
\usepackage{overpic}
\usepackage{color}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{changepage}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    filecolor=magenta,      
    urlcolor=cyan,
}

\input{head/settings_custom.tex}  
\input{htrack/sec/tweaks.tex}
\input{hmodel/sec/tweaks.tex}
\input{hmodel/sec/math.tex}
\input{hmodel/sec/videotimes.tex}
\input{honline/sec/tweaks.tex}
\input{honline/sec/math.tex}

\begin{document}
\frontmatter
\input{head/titlepage.tex}
%\include{head/dedication}
%\input{head/acknowledgements}
%\include{head/preface}
%\include{head/abstracts}

\tableofcontents
%\addcontentsline{toc}{chapter}{List of figures} % adds an entry to the table of contents
%\listoffigures
%\addcontentsline{toc}{chapter}{List of tables} % adds an entry to the table of contents
%\listoftables

\setlength{\parskip}{1em}

\mainmatter


\chapter{Introduction}

\section{Motivation}

\subsection*{Applications}

Tracking and humans in motion is a fundamental problem in computer graphics and computer vision. A particularly important question is how to accurately reconstruct the shape and articulation of human hands. Firstly, because in our everyday life we interact with the surrounding environment using our hands. Secondly, hand motion is a crucial component of non-verbal communication. The digital world applications of hand tracking follow from these two functions of hands in real world.

\paragraph{Performance capture} Performance capture is essential in film and game production for pre-visualization, where motion can be transferred in real-time to a virtual avatar. This allows directors to plan shots more effectively, reduce turn-around times and hence costs. 

\paragraph{Remote communication} Being an important part of our body language, hand motion  plays an important role in the animation of humanoid avatars. The first steps towards commercial avatar-based communication were already made by 
%
\textit{Microsoft Holoportation}
\footnote{\href{https://www.microsoft.com/en-us/research/project/holoportation-3/}{Microsoft Holoportation}, accessed on 27.11.2017}
%
and \textit{Apple Animoji} 
\footnote{\href{https://support.apple.com/en-us/HT208190}{Apple Animoji}, accessed on 27.11.2017}.

\paragraph{Gesture control} Gesture control, a simplified version of hand tracking, becomes increasingly popular as a replacement of remote control for home appliances. It is currently used in such consumer products as 
%
\textit{Samsung Smart TV}
\footnote{\href{http://www.samsung.com/ph/smarttv/motion_control.html}{Samsung Smart TV}, accessed on 27.11.2017}
%
 and \textit{Singlecue}
 \footnote{\href{https://singlecue.com/}{Singlecue}, accessed on 27.11.2017}.
%
 A few other similar products are currently under development.

\paragraph{Virtual interaction} Recently the field of virtual and augmented reality (VR/AR) has made a large step forward. A number of VR/AR headsets was released, including 
\textit{Oculus}, \textit{Vive}, \textit{Samsung Gear VR}, \textit{Microsoft Hololens}, \textit{PlayStation VR}, \textit{Google Daydream}, \textit{Microsoft Mixed Reality Headset}, \textit{Intel Project Alloy} and \textit{Meta 2 AR Headset}. The technology is incomplete without providing the user a way to interact with virtual environment. Most of the listed headsets started with dedicated controller devices. However, there in a notion in the field that hand control is more desirable.
%
\textit{Microsoft Hololens}
\footnote{\href{https://www.microsoft.com/en-us/hololens}{Microsoft Hololens}, accessed on 27.11.2017},
%
\textit{Meta 2 AR Headset}
\footnote{\href{https://www.metavision.com/}{Meta 2 AR Headset}, accessed on 27.11.2017} and
%
\textit{Intel Project Alloy} 
\footnote{\href{https://newsroom.intel.com/press-kits/project-alloy/}{Intel Project Alloy}, accessed on 27.11.2017}
%
are already released in a hand-controlled version and the other main manufactures are also currently developing similar technology.
There are several reasons why VR/AR helmets benefit from hand control. Firstly, according to the user study conducted by 
%
\textit{Leap Motion}
\footnote{\href{http://blog.leapmotion.com/image-hands-bring-your-own-hands-into-virtual-reality/}{Leap Motion Blog}, accessed on 27.11.2017},
%
interacting with your own hands creates a more immersive experience. Secondly, it takes time to get used to the controller device and to remember the functionality assigned to each button. Moreover, hands control can potentially more expressive and subtle than a dedicated controller device.

The commercial hand control devices are still an immerging technology, because hand tracking is challenging and remains a research problem. This was even more so at the start of my doctoral studies. The challenges are described below.

\subsection*{Requirements}

Any consumer application relies on robustness of the tracker. However, the applications listed above have different requirements to the precision, efficiency and output format of hand tracking algorithm.

\textit{Gesture control} system is only required to determine type of gesture, thus even inferring the exact hand pose is not necessary. For \textit{performance capture} it is acceptable have under real time performance. In \textit{remote communication}, if hand motion is re-targeted to an avatar, it only requires tracking joint positions as opposed to hand surface.

\textit{Virtual interaction} is the most demanding, yet most promising application. It requires precise tracking of hand movements. As explained below, precise tracking is only possible if the model is precisely calibration to the user. Moreover, to be suitable for consumer application, it is undesirable for calibration to take long time or require user input. Physically plausible interaction with virtual object requires the system to infer not just hand joint positions, but the volumetric hand model surface.

\subsection*{Challenges}

\paragraph{Tracking challenges} Accurate hand tracking with a non-invasive sensing device in realtime is a challenging scientific problem. Human hands are highly articulated and therefore require models with sufficiently many degrees of freedom to adequately describe the corresponding motion space. Hand motion is often fast and exhibits intricate geometric configurations with complex contact patterns among fingers. With a single-camera RGBD setup, we are faced with incomplete data due to self-occlusions and high noise levels.  

\paragraph{Calibration challenges} Hight precision model based tracking is impossible without calibrating the model to the specific user. The main challenge comes from the fact that tracking and calibration procedures are interdependent. High quality tracking requires good calibration and, to accurately calibrate the model, the motion needs to be precisely tracked. Moreover, hand calibration is bound to consider multiple frames, since from a single frame only a subset of the shape degrees of freedom can be estimated. For example, it is difficult to estimate the length of a phalanx when observing a straight finger.

\subsection*{Setup}

\paragraph{Tracking setup} Over the past two decades a number of techniques have been explored to address hand tacking problem, from expensive and unwieldy marker-based mocap \cite{mocapsurvey} to instrumented gloves \cite{dipietro2008survey} as well as imaging systems \cite{erol2007vision}. Multi-camera imaging systems can recover the hand pose and hand-objects interactions with high accuracy \cite{ballan2013salient}, but the only system capable to approach interactive applications is the 10Hz system of \cite{sridhar2013multicam}. Conversely, in this paper we focus on hand motion tracking with a single RGBD sensor (e.g. Intel RealSense or Microsoft Kinect), commonly predicted to be readily available in a typical AR/VR consumer experience.
We focus on hand motion tracking with a single RGBD sensor (e.g. Intel RealSense or Microsoft Kinect), commonly predicted to be readily available in a typical AR/VR consumer experience. This setup does not require the user to wear a glove or markers. Such single-camera acquisition is particularly advantageous as it is cheap, does not require any sensor calibration, and does not impede user movements.

\paragraph{Tracking: discriminative vs. generative} Modern systems for real-time tracking from RGBD data \cite{sridhar2015fast,sharp2015accurate} rely on a combination of discriminative approaches like \cite{keskin2012hand}, and generative approaches such as \cite{oiko2011hand}. The per-frame re-initialization of discriminative methods prevents error propagation by offering a continuous recovery from tracking failure. As these discriminative models are learnt from data, they are typically limited in their precision by dataset annotation accuracy. Annotating joint locations is challenging because it needs to be done in 3D and because the joints are situated inside of the hand. These difficulties reflect on the labeling quality. Therefore, generative models are used to refine the estimate by aligning a geometric template of the user hand to the measured point cloud as well as to regularize its motion through time. It is not surprising that the quality of the template directly affects the quality of pose refinement.

\paragraph{Calibration setup} The process of accurately generating a user-specific tracking model from input data is referred to in the literature as calibration or personalization. Calibrating a template from a set of static poses is a standard component in the workflow of facial performance capture \cite{weise2011realtime,cao2015facial}, and the work of \cite{taylor2014user} pioneered it within the realm of hand tracking. However, current methods such as \cite{taylor2016joint} suffer a major drawback: the template must be created during a controlled calibration stage, where the hand is scanned in several static poses (i.e. offline). While appropriate for professional use, a calibration session is a severe drawback for seamless deployment in consumer-level applications. 


\section{Contributions}

This dissertation is based on and uses parts of the following papers published in
the course of my PhD:

\begin{adjustwidth}{2.5em}{0pt}
\textsc{Tagliasacchi A., Schroeder M., Tkach A., Bouaziz S., Botsch M., Pauly
M.}: Robust articulated-icp for real-time hand tracking. \textit{Computer Graphics
Forum(Proc. of the Symposium on Geometry Processing) (2015)}.

\textsc{Tkach A., Pauly M., Tagliasacchi A.}: Sphere-meshes for real-time hand
modeling and tracking. \textit{In ACM Trans. Graph. (Proc. SIGGRAPH Asia) (2016)}.

\textsc{Tkach A., Tagliasacchi A., Remelli E., Pauly M., Fitzgibbon A.}: Online generative model personalization for hand tracking. \textit{ACM Transactions on Graphics (Proc. SIGGRAPH Asia). 2017}.
\end{adjustwidth}

In summary, the contributions of this dissertation are:
\begin{itemize}

\item \paragraph{Robust realtime model-based hand tracking algorithm}
We developed a robust model-based hand tracking algorithm that efficiently integrates data and regularization priors into a unified realtime solver running at 60 FPS. The fist key component of the algorithm is an efficient combined 2D/3D registration method to align the 3D hand model to the acquired depth map and extracted silhouette image. The second key feature is a new way of computing data-to-model correspondences that accounts for occlusions and significantly improves the robustness of the tracking

\item \paragraph{Sphere-meshes model for efficient and accurate hand shape representation}
We presented sphere-meshes hand model and demonstrated that it provides superior hand tracking performance for single-view depth sensors. We introduced an optimization approach that allows adapting our tracking model to different human hands with a high level of accuracy. The improved geometric fidelity compared to existing representations leads to quantifiable reductions in registration error and allows accurate tracking even for intricate hand poses and complex motion sequences that previous methods have difficulties with. At the same time, due to a very compact model representation and closed-form correspondence queries, our generative model retains high computational performance, leading to sustained tracking at 60 FPS.

\item \paragraph{Online hand model calibration} 
We introduced a principled way of integrating per-frame information into an online real-time pose/shape tracking algorithm: one that estimates the hand’s pose, while simultaneously refining its shape. That is, as more of the user’s hand and articulation is observed during tracking, the more the tracking template is progressively adapted to match the performer, which in turns results in more accurate motion tracking. Our technique automatically estimates the confidence in per-frame parameter computations, and leverages the this information to build a tracking model that selectively accumulates confident parameter estimates over time. Assuming a reasonable performance by the user, our system typically constructs a fully calibrated model within a few seconds, while simultaneously tracking the user in real time. 

\item \paragraph{Open Source} Another important contribution is that we fully disclosed our source code. To the best of our knowledge, no other freely available implementation is available, and we believe that publishing our code will not only ensure reproducibility of our results, but also facilitate future research in this domain.

\end{itemize}

\section{Overview}

The remainder of the thesis describes our steps in solving the problem of precise model-based hand tracking. The problem consists of two inter-dependent components: accurate tracking and accurate calibration.

The Chapter \ref{ch:tracking} describes our initial hand tracking system that uses cylinders hand model. 

Section \ref{sec:htrack-intro},
Section \ref{sec:htrack-related}, ...

In Section  \ref{sec:htrack-overview} we address the challenges of robust hand tracking by proposing a regularized articulated ICP-like optimization that carefully balances data fitting with suitable priors. Our data fitting performs a joint 2D-3D optimization. The 3D alignment ensures that every point measured by the sensor is sufficiently close to the tracked model. Simultaneously, as we cannot create such constraints for occluded parts of the hand, we integrate a 2D registration that pushes the tracked model to lie within the sensor visual hull. In Section \ref{sec:optimization} we detail a carefully chosen set of priors, that regularize the solution to ensure the recovered pose is plausible. After discussing some implementation details in Section \ref{sec:implementation}, we analyze tracking performance by providing a comparison to several state-of-the art solutions in Section \ref{sec:eval}.

The Chapter \ref{ch:sphere-meshes} addresses the choice of hand model representation that is suitable both for efficient tracking and for accurate calibration.

Section \ref{sec:hmodel-intro},
Section \ref{sec:related}, ...

In Section \ref{sec:tracking} we detail how our novel formulation fits into previous generative real-time hand tracking technique, while still enabling efficient correspondence computation. Section \label{sec:modeling} explains how we build our template model from 3D scans acquired either through multi-view stereo or from depth maps. In Section \label{sec:results} we analyze the performance of our model for realtime tracking and provide comparisons to the state-of-the-art. 

In Chapter \ref{ch:online} we reconsider state offline calibration approach, aiming to enhance user experience and push calibration quality further. 

Section \ref{sec:honline-intro},
Section \ref{sec:honline-related}, ...

In Section \ref{sec:technical} we describe our joint calibration and tracking algorithm, which combines the Levenberg-style optimization of previous hand trackers with the uncertainty maintenance framework of Kalman filtering. In Section \ref{honline-eval}, to evaluate the technical validity of our approach we corroborate the formulation of our optimization on a synthetic 3D dataset; analyze its robustness through randomly perturbing the algorithm initialization; and attest how our method achieves state-of-the-art performance on publicly available datasets. In Section \ref{honline-implementation} introduce Kalman filter with its extensions and derive the equivalence of the proposed online calibration scheme with a recent tool from control theory – the Levenberg-Marquardt Kalman Filter.

\section{Related Works}

In this section we summarize main works in real-time single view hand tracking from depth input. The works from the other areas relevant to the subsequent chapters of this thesis are reviewed in the related literature sections of the corresponding chapters. 

Tracking algorithms can be roughly divided into two main classes - discriminative and generative.
\vspace{-1.5em}
\begin{itemize}
\item \textit{Discriminative} methods directly predict hand pose from image features. State of the art approaches learn the mapping between the image and hand pose from the annotated training data. The most widely used learning algorithms are Random Decision Forest (RDF) \cite{keskin2012hand} and Convolutional Neural Networks (CNN) \cite{tompson2014real}. Discriminative algorithms regress small number of key features, like joint positions or angles, as opposed to volumetric hand model. The predicted hand pose can afterwards be used to drive a hand model, however the surface of that model is not optimized to align with the data. 
%
\item \textit{Generative} methods minimize the discrepancy between the had model and the input data by solving a data-model alignment problem. The main algorithms used for this task are Gradient Descent \cite{taylor2016concerto} and Particle Swarm Optimization (PSO) \cite{oiko2011hand}, there are also some new works that use CNN \cite{simon2017hand}, \cite{zimmermann2017learning}. Gradient Descent and PSO require initialization, which is either obtained from hand pose at previous frame or from a discriminative method.
\end{itemize}

\subsection{Discriminative Methods}

\hspace{-0.4em}
\textbf{\cite{keskin2012hand}} estimate hand pose by predicting the hand part labels probabilities for each pixel. The labels prediction is done using an RDF. The centers of the hand parts are inferred by representing each label with a gaussian and finding the optimum on the resulting surface. This is under assumption that the pixel with maximum probability value for the given hand part is situated in the center of that hand part. The hand skeleton is obtained by connecting the joints according to their configuration in the hand. To improve performance, the training set is split in clusters of similar hand poses. The results from different clusters are aggregated by an expert network.
   
\hspace{-0.4em}
\textbf{\cite{tang_cvpr14}} present a method similar to the one introduced by \cite{keskin2012hand}. Differently from the former, instead of using an RDF for of predicting hand parts, they adopt a novel algorithm -  Latent Regression Forest (LRF). In LRF the non-leaf nodes correspond to groupings of hand parts. The method performs structured coarse-to-fine search, starting with entire hand and recursively splitting it, until locating all the skeletal joints. This work has superior performance with respect to \cite{keskin2012hand}, one of the reasons is greater robustness to occlusions.

\hspace{-0.4em}
\textbf{\cite{tompson2014real}} pioneered using CNNs for discriminative hand tracking. Their work (and numerous subsequent methods) are enabled by the automatically labeled dataset that they have constructed. The authors trained a CNN to generate a set of heat-map images for key hand features, taking multi-resolution depth images as an input. At each resolution the network contains two convolution layers; each convolution is followed by RELU and max pooling. The concatenated outputs of convolution layers are fed to two fully connected layers. The final kinematically valid hand pose is obtained by applying an inverse kinematic model on the heat-maps.

\hspace{-0.4em}
\textbf{\cite{sun2015cascaded}} use cascaded regression for predicting hand pose. In cascaded regression framework, the pose is estimated iteratively by a sequence of regressors. Each regressor uses the output of the previous one, progressively decreasing the error. The regressors are learned with RDF. The authors modify widely used for RDF offset features to make them invariant to 3D transformations. They also propose a hierarchical approach to regress hand pose. Firstly, the palm transformation is regressed. The inverse of this transformation is afterwards applied to the fingers before estimating their poses. This approach is shown to perform better than estimating the pose holistically, as it reduces appearance variations for the fingers.

\hspace{-0.4em}
\textbf{\cite{tang2015opening}} 
propose to estimate hand pose hierarchically starting with the parameters at the base of hand kinematic chain and inferring the parameters at each next layer conditioned on the previous layer (layer 1 – wrist translation, layer 2 – wrist rotation, and so on along the kinematic chain). For efficiency they formulate cost function in terms of joint positions only. Advantageously, evaluation of this cost function does not require rendering the model or computing closest point correspondences. Moreover, this cost function can also be evaluated for partial poses. The proposed hierarchical optimization framework generates several samples of the partial pose at each layer, the sample with the minimal value of cost function is then selected. To generate the samples, the authors train an RDF for predicting partial poses, they use standard features for RDF on depth images. The system generates multiple hypotheses using the described approach, the final pose is selected by evaluating “golden energy” suggested by \cite{sharp2015accurate}. This approach outperforms the other woks that use hierarchical hand pose estimation algorithms, such as \cite{tang_cvpr14} and\cite{sun2015cascaded}.

\hspace{-0.4em}
\textbf{\cite{li20153d}} 
extend the work of \cite{keskin2012hand} and Tang by proposing another variant of RDF. Similarly to Tang 2014, the method performs structured coarse-to-fine search, starting with entire hand and splitting it recursively to joints. Differently from Tang 2014 the division hierarchy of hand parts may not be the same for different poses. The work achieves superior performance on the Tang et al. dataset.

\hspace{-0.4em}
\textbf{\cite{oberweger2015hands}} 
compare several CNN architectures and find that the best performance is given by a deeper architecture that takes depth images at several scales as an input. The rationale is that using multiple scales helps capturing contextual information. The authors also propose to regress hand pose parameters in a lower dimensional subspace. After the initial estimation phase follows a refinement step. To refine the location estimate provided by the first stage they use a different network for each joint. Each network looks at several patches of different centered on the predicted joint location. The refinement step is repeated several times, each iteration is centered on a newly predicted location.

\hspace{-0.4em}
\textbf{\cite{oberweger2015feedback}} 
Oberweger 2015 b design a convolutional neural network capable of directly synthesizing hand depth images with a purpose of replacing hand model for hybrid tracking. As a first step they use a first CNN to predict an initial pose from the depth input. The initial pose is used to synthesize a depth image. The synthesized image and the input image are feed to updater CNN. The updater learns to predict updates, which would improve the pose estimate, given the input and synthesized depth. This process is repeated for several iterations. The synthesizer network consists of several fully-connected layers followed by several unpooling and convolution layers. The updater network has a Siamese architecture. It consists of two identical paths of several convolutional layers. The final feature maps are concatenated and fed into a fully connected network.

\hspace{-0.4em}
\textbf{\cite{ge2016robust}} 
propose to project the input depth image onto orthogonal planes and use the resulting views to predict for 2D heat-maps of joint locations on each plane. These 2D heat-maps are then fused to produce final 3D hand pose. The fusion step is expected to correct the imprecisions using the predictions from complimentary viewpoints. They use a multi-resolution CNN on each view with architecture similar to Tompson et al. Given the 2D heat maps from three views, they find the hand pose parameters in a lower dimensional PCA subspace, such that the total heat map confidence at the joint locations on the three views is maximized. 

\hspace{-0.4em}
\textbf{\cite{sinha2016deephand}} 
exploit activation features from a hidden layer of a trained CNN. The assumption is that augmenting an output activation feature by a pool of its nearest neighbors brings more reliable information about hand pose. Drawing on the fact that CNNs are less robust for regression than for classification, Sinha et al. compute the activation features from classifying joint angles into bins with a CNN. Since the number quantized hand poses is very large, they propose two stage classification. On the first stage global hand rotation is classified. Next, for each rotation bin, five separate CNNs are trained to classify the poses of the fingers. In run time, given the activation features, a pool of their nearest neighbors is efficiently retrieved from a database. The final hand pose is computed from the assumption that a matrix of stacked neighboring activation features concatenated with stacked corresponding hand poses has a low rank. The unknown current hand pose is computed by matrix completion.

\hspace{-0.4em}
\textbf{\cite{zhou2016model}} 
integrate domain knowledge about hand motion into a CNN, by adding a non-parametric layer that encodes a forward kinematic mapping from joint angles to joint locations. Since forward kinetic function is differentiable, it and can be used in a neutral network for gradient-descent like optimization. This approach guarantees that the predicted hand pose is valid. The remaining network architecture is similar of Oberweger 2015.

\hspace{-0.4em}
\textbf{\cite{madadi2017end}} 
propose using a hierarchical tree-like CNN mimicking the kinematic structure of human hand. The branches of the network are trained to become specialized in predicting the locations of subsets of hand joints (local pose), while the parameters closer to the tree root are shared for all hand parts. The network contains a loss term for each local pose. Additionally, the outputs of the tree branches are concatenated and feed to the fully-connected layer for estimating the final pose. The authors argue that this step allows to learn higher order dependencies among joints. The loss function also contains the terms that penalize predicting joint locations outside of data hull and encourage all joints from one finger to be co-planar.

\hspace{-0.4em}
\textbf{\cite{mueller2017real}} 
present a method for predicting hand pose in egocentric view. There system is designed for hand-object interaction scenarios which and is robust to occlusions. They estimate hand pose in several steps. Firstly, to localize the hand, a heat map of the hand root position is regressed. Given the hand root, the image is normalized and feed into a joint regression network. This network outputs 2D heat maps and 3D positions of the joints. As the last step, a kinematically valid hand pose is computed by optimizing a sum of energies cost function, which includes the closeness to regressed joint locations, joint limits and temporal smoothness term. The networks were trained on synthetic data generated by accurately tracked hand motion with existing tracker and retargeting it to a virtual hand model.

\hspace{-0.4em}
\textbf{\cite{oberweger2017deepprior++}} 
extend their work from 2015 and carry out an extensive evaluation to show that it achieves better or similar performance with all recent methods on three main benchmarks of hand tracking. They introduce to following improvements: the training data is augmented online up to 10M samples by translation, rotation and scaling. The second enhancement is training a CNN that regresses hand root for accurate hand localization. The new pose network architecture is similar to ResNet: a convolution layer is followed by four residual modules followed by several fully connected layers with dropout.

\hspace{-0.4em}
\textbf{\cite{wan2017crossing}} 
propose a framework for learning from unlabeled data in a semi-supervised manner. They learn a shared latent space where each point can be mapped both to a synthetic depth image and to the corresponding hand pose parameters. The hand pose is regressed by training a discriminator to predict a posterior of the latent pose given the input depth map. The depth map generator and discriminator are trained jointly in order to improve generalization. To avoid overfitting during posterior estimation they add several more loss terms that share first several convolutional layers with pose estimation.

\subsection{Generative Methods}

\hspace{-0.4em}
\textbf{\cite{oiko2011hand}} 
present a generative tracking approach. Their algorithm minimizes the difference between the sensor data and the rendered capsules model. The optimization is performed using Particle Swarm Optimization. The method runs at 15 fps on GPU and does not include re-initialization component in case of tracking failure.

\hspace{-0.4em}
\textbf{\cite{melax2013dynamics}} 
show compelling 60 fps realtime performance using gradient-based optimization. The optimization is expressed as a convex rigid body simulation, and numerous heuristics for re-initialization were employed to avoid tracking failures. Melax et al. introduce a convex polyhedral model and track it with rigid body dynamics solver. The rigid bodies from the model are constrained to come into contact with point cloud. The hand parts are attached together with constraints of larger strength. Thus, in contrast the majority of model based systems, this technique does not use Inverse Kinematics. As in ICP, each data point adds a constraint on the closest articulated component of the hand. The model is also constrained to stay within 3D hull of the point cloud by adding collision planes constraints on the boundaries of the convex hull.

\hspace{-0.4em}
\textbf{\cite{oikonomidis2014evolutionary}} 
introduce a more advanced sampling strategy that improves tracking efficiency without compromising quality. They sample the space using quasi-random sequence that covers multi-dimensional spaces better than random. However, gradient-based optimization approaches converge faster and more accurately than PSO when close to the solution.

\hspace{-0.4em}
\textbf{\cite{qian2014realtime}} 
modify PSO algorithm employed by Oikonomidis et al. by adding a gradient-based component to it. Each particle takes an additional ICP like gradient descent step in each PSO generation. This is intended to combine advantages and mitigate drawbacks of PSO and ICP. The authors demonstrate that their system has superior performance to Oikonomidis et. al. The presented system is hybrid, it uses a spheres model for ICP-PSO optimization and detects fingertips with flood fill for re-initialization.  Apart from closeness of the model to the data, the cost function also includes a term that constrains the model to lie within sensor visual hull and behind the data.

\hspace{-0.4em}
\textbf{\cite{schroder2014real}} 
formulate the optimization in a subspace of likely hand poses, rather than resorting to reinitialization for robustness. They capture a dataset of human hand movements with a Vicon motion tracking system as the ground truth for deriving natural hand synergies based on principal component analysis. While the lower number of optimization variables leads to efficient computations, tracking accuracy can be limited by the reduced pose complexity induced by the subspace. Schroder et al. use a cylinder hand model driven by Inverse Kinematics and employ ICP algorithm for aligning the model with the data.

\hspace{-0.4em}
\textbf{\cite{fleishman2015icpik}} 
present a system that uses capsules and model and ICP-IK algorithm for data-model alignment. The authors use RF classifier to label data pixels with hand parts. For robustness, the system generates several hypotheses of hand pose from the labeled data. In the final step, they apply ICP-IK algorithm to each skeleton hypothesis (with each finger being straight or bent). The closed point correspondences are only created between the same parts of the data and model. The authors claim that ICP-IK algorithm gives superior performance wrt PSO.

\hspace{-0.4em}
\textbf{\cite{poier2015hybrid}} 
regress joint locations using and RDF. The authors consider several top predictions for each joint along with their confidence score. The kinematic parameters of a 3D hand model are found by selecting a proposal for each joint location, such that the selected locations for all joints form an anatomically valid pose. They apply PSO for optimizing the corresponding cost function. For efficiency, the authors split the full PSO problem into subproblems solving for the pose of each finger independently. Differently from \cite{oiko2011hand} this approach does not require rendering the model, thus it runs on CPU.

\hspace{-0.4em}
\textbf{\cite{sharp2015accurate}} 
introduce a hybrid approach minimizes “golden energy” - the reconstruction error between a rendered 3D hand model and the observed depth image. The comparison can be done accurately since this is the first approach to use a detailed hand model instead of spheres/cylinders. The model is not differentiable; thus the authors use PSO for optimization. The RDF re-initializer runs in two stages, the first stage only deals with predicting quantized global hand rotation, the second stage refines the rotation and regresses the pose. The system is robust and works well at distance of several meters and moving camera scenarios.

\hspace{-0.4em}
\textbf{\cite{sridhar2015fast}} 
encode the model with a predefined mixture of gaussians. The data is also represented as a mixture of gaussians. This is done through decomposing the depth image into regions of homogeneous depth using quadtree and fitting a Gaussian to each region. The authors optimize the closeness of model to the data with gradient descent. Gaussian mixture representation allows, instead of computing closest point correspondences, to match data mixtures of gaussians with the model. For robustness the system generates multiple hypotheses of hand pose and chooses the best based on pose fitting energy. One of the hypothesis comes from an RDF hand parts classifier. For that hypothesis another type of energy is optimized: each gaussian in the data is given a part label which is most frequent among its pixels. The model is aligned with the data according to hand part labels.

\hspace{-0.4em}
\textbf{\cite{taylor2016concerto}} 
present a continuous registration framework for tracking hands with triangular meshes. The control mesh is augmented with a continuous Loop subdivision surface that provides gradients for optimization. Similar to Tagliasacchi et al they define a differentiable cost function as a weighted sum of several terms, including data energy, joint limits, pose prior, temporal prior, etc. For the data energy term Taylor et al introduce an alternative to ICP algorithm. To compute closest point correspondences, they define a set of corresponding variables that are optimized jointly with model pose. Compared to ICP, the proposed algorithm requires less iterations and has a wider convergence basin. 

\hspace{-0.4em}
\textbf{\cite{taylor2017articulated}} 
introduce a new hand model representation that avoids the compromise between efficiency and accuracy. This is achieved by constructing an articulated signed distance function that provides closed form distances to the model surface and is differentiable. In more details the hand model is driven by a linear blend skinned tetrahedral mesh that deforms a precomputed signed distance field (detailed hand surface) into a given pose. The closest point correspondences are computed in efficient and parallelizable manner, this allows the system to run at ultra-high frame rates of GPU (1000Hz). Due to its efficiency and robustness of this system becomes the first one to that accurately tracks complex interaction of two hands.




\chapter{Robust Articulated-ICP for Real-Time Hand Tracking} \label{ch:tracking}
\input{htrack/sec/teaser.tex}
\input{htrack/sec/abstract}
\input{htrack/sec/intro}
\input{htrack/sec/related}
%\input{htrack/sec/overview}
%\input{htrack/sec/technical}
%\input{htrack/sec/technical2}
%\input{htrack/sec/eval}
%\input{htrack/sec/conclusions}
%\input{htrack/sec/appendix}
%\input{htrack/sec/ack}
%
%\chapter{Sphere-Meshes for Real-Time Hand Modeling and Tracking} \label{ch:sphere-meshes}
%\input{hmodel/fig/teaser/item.tex}
%\input{hmodel/sec/abstract}
%\input{hmodel/sec/intro.tex}
%\input{hmodel/sec/related.tex}
%\input{hmodel/sec/overview.tex} 
%\input{hmodel/sec/corresp.tex}
%\input{hmodel/sec/fitting.tex}
%\input{hmodel/sec/results.tex}
%\input{hmodel/sec/discussion.tex}
%\input{hmodel/sec/conclusion.tex}
%\input{hmodel/sec/acks.tex}
%
%\chapter{Online Generative Model Personalization for Hand Tracking} \label{ch:online}
%\input{honline/fig/teaser/teaser}
%\input{honline/sec/0_abstract}
%\input{honline/sec/1_introduction}
%\input{honline/sec/2_related}
%\input{honline/sec/3_technical}
%\input{honline/sec/4_evaluation}
%\input{honline/sec/5_conclusions}
%%\input{honline/sec/6_acks}
%\input{honline/sec/appendix}

\bibliographystyle{apalike}
\bibliography{thesis}

%\include{tail/biblio}
%\include{tail/cv}

\end{document}
